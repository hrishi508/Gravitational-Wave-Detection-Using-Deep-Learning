{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Dropout, MaxPool1D, ReLU, Flatten\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_class = 5000\n",
    "no_of_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_df = pd.read_csv(\"~/SOP/Gravitational Wave Detection Using Deep Learning/data/Final_Merged_Noise_Reduced_No_Abs.csv\", header=None)\n",
    "noise_X = noise_df.values.astype(float)\n",
    "# noise_y = np.zeros((samples_per_class , 1)).astype(float)\n",
    "\n",
    "\n",
    "data_df = pd.read_csv(\"~/SOP/Gravitational Wave Detection Using Deep Learning/data/Final_BBH_Merged_Noise_Signal_Reduced_No_ABS.csv\", header=None)\n",
    "data_X = data_df.values.astype(float)\n",
    "# data_y = np.ones((samples_per_class , 1)).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "[[ 1.55952588e-19  9.48930874e-20 -2.03908807e-19 ... -1.42081173e-19\n",
      "   1.56639750e-19  2.06323581e-19]\n",
      " [ 1.68342261e-19 -2.32436731e-20  2.01099010e-19 ...  4.92764727e-20\n",
      "   6.88449772e-21  7.69596876e-20]\n",
      " [-1.77127428e-21 -1.16860154e-19 -2.75680221e-20 ...  9.51481870e-20\n",
      "   1.40486501e-19  1.42635313e-19]\n",
      " ...\n",
      " [ 1.24066490e-19  2.02760022e-19  7.02894628e-20 ... -1.74086976e-21\n",
      "   7.28379795e-20  6.74019667e-20]\n",
      " [-2.50812218e-19 -3.65408728e-19 -2.59820380e-19 ... -1.21963367e-19\n",
      "   4.15912167e-20 -2.15545938e-19]\n",
      " [-1.66340990e-19 -2.05033650e-20 -2.51719522e-19 ... -1.02534509e-19\n",
      "   1.37499247e-19 -3.59096226e-20]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((noise_X, data_X), axis=0)\n",
    "# Y = np.concatenate((noise_y, data_y), axis=0)\n",
    "\n",
    "print(len(noise_df.index))\n",
    "print(X)\n",
    "# print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging\n",
    "\n",
    "# mnist = tf.keras.datasets.mnist\n",
    "# (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# print(y_train)\n",
    "# y_train = tf.keras.utils.to_categorical(y_train)\n",
    "# y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "# print(y_train)\n",
    "# print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Alternate way of creating y for the dataset\n",
    " \n",
    "y = [int(i/samples_per_class) for i in range(samples_per_class*no_of_classes)]\n",
    "y = tf.keras.utils.to_categorical(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging\n",
    "\n",
    "# x = [[0,0],[1,1]]\n",
    "# a = [[0],[1]]\n",
    "# x = np.hstack((x, a))\n",
    "# # x\n",
    "# x, a, _ = np.split(x, [len(x[0])-1, len(x[0])], axis = 1)\n",
    "# x, a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-8.05570352e-20 -1.76292189e-19 -2.78905620e-19 ...  4.96658190e-20\n",
      "  -1.37709316e-19  2.02123267e-20]\n",
      " [ 1.67836074e-19  1.01601424e-19 -1.15978384e-20 ...  4.92470310e-20\n",
      "   4.50740090e-20 -1.09826435e-19]\n",
      " [ 3.70501874e-20  3.29739776e-20  4.96176489e-20 ...  1.14183882e-20\n",
      "  -1.08185780e-19 -1.65597939e-19]\n",
      " ...\n",
      " [-1.81626457e-20  1.40824971e-20  1.49160302e-19 ... -1.48777901e-19\n",
      "  -1.08221047e-19  2.71519506e-20]\n",
      " [-2.68313188e-19 -9.82505725e-20 -3.11455516e-19 ...  3.27260202e-20\n",
      "   5.94208046e-20 -1.45863531e-19]\n",
      " [-5.73372557e-20 -4.05587457e-20 -1.05371917e-19 ... -1.13198792e-19\n",
      "   7.55365287e-20 -2.39195113e-20]]\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# shuffling the data\n",
    "\n",
    "X = np.hstack((X, y))\n",
    "np.random.shuffle(X)\n",
    "\n",
    "X, y, _ = np.split(X, [len(X[0])-no_of_classes, len(X[0])], axis = 1)\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.66502223e-01 -1.07359821e+00 -1.74433671e+00 ...  5.02581170e-01\n",
      "  -1.36933782e+00  2.26637884e-01]\n",
      " [ 1.12471825e+00  7.27310905e-01 -1.06618344e-03 ...  4.98396855e-01\n",
      "   4.52394582e-01 -1.06678426e+00]\n",
      " [ 2.86896372e-01  2.82565906e-01  3.98155846e-01 ...  1.20432591e-01\n",
      "  -1.07508789e+00 -1.62151193e+00]\n",
      " ...\n",
      " [-6.68001951e-02  1.60138337e-01  1.04733176e+00 ... -1.48016600e+00\n",
      "  -1.07543938e+00  2.95662405e-01]\n",
      " [-1.66927888e+00 -5.67844024e-01 -1.95661364e+00 ...  3.33327448e-01\n",
      "   5.95383678e-01 -1.42522492e+00]\n",
      " [-3.17754987e-01 -1.93968081e-01 -6.12621844e-01 ... -1.12467792e+00\n",
      "   7.56003017e-01 -2.12316594e-01]]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging\n",
    "\n",
    "# x = [[0,0],[1,1]]\n",
    "# y = np.expand_dims(x, axis=-1)\n",
    "# print(y, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.expand_dims(X, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(64, 32, input_shape = (16384,1)))\n",
    "model.add(MaxPool1D(4, 4))\n",
    "model.add(ReLU())\n",
    "model.add(Conv1D(128, 64))\n",
    "model.add(MaxPool1D(4, 4))\n",
    "model.add(ReLU())\n",
    "model.add(Conv1D(256, 64))\n",
    "model.add(MaxPool1D(4, 4))\n",
    "model.add(ReLU())\n",
    "model.add(Conv1D(512, 128))\n",
    "model.add(MaxPool1D(4, 4))\n",
    "model.add(ReLU())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))                         # dropout rate not givem\n",
    "model.add(Dense(2, activation=tf.nn.softmax))\n",
    "\n",
    "# learning rate not given\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16384, 1) (10000, 2)\n",
      "(1600, 16384, 1) (1600, 2)\n",
      "(400, 16384, 1) (400, 2)\n"
     ]
    }
   ],
   "source": [
    "# print(X1.shape, y.shape)\n",
    "# X2 = X1[:1600, :]\n",
    "# y2 = y[:1600, :]\n",
    "\n",
    "# X3 = X1[1600:2000, :]\n",
    "# y3 = y[1600:2000, :]\n",
    "# print(X2.shape, y2.shape)\n",
    "# print(X3.shape, y3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16384) (10000, 2)\n",
      "(10, 16384, 1) (10, 2)\n",
      "(2, 16384, 1) (2, 2)\n"
     ]
    }
   ],
   "source": [
    "# print(X.shape, y.shape)\n",
    "# X2 = X[:10, :]\n",
    "# y2 = y[:10, :]\n",
    "\n",
    "# X3 = X[10:12, :]\n",
    "# y3 = y[10:12, :]\n",
    "\n",
    "# X2 = np.expand_dims(X2, axis=-1)\n",
    "# X3 = np.expand_dims(X3, axis=-1)\n",
    "# print(X2.shape, y2.shape)\n",
    "# print(X3.shape, y3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, batch_size=32, epochs=10) #, validation_data=(X3, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8356d31dd4c50017c60d528a0c40a4935f9d791f54e24c10e32e3a312d5b6882"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
