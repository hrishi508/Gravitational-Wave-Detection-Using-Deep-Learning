{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs_OpQDIxDQW",
        "outputId": "2c318b83-3400-4513-e1df-ec167dad8688"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5inbyEdw1A7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, Dropout, MaxPool1D, ReLU, Flatten\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# RESNET IMPORTS\n",
        "import time\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('agg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hF5cR2kzw1A-"
      },
      "outputs": [],
      "source": [
        "samples_per_class = 5000\n",
        "no_of_classes = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzCUizYfw1A_"
      },
      "outputs": [],
      "source": [
        "noise_df = pd.read_csv(\"/content/gdrive/MyDrive/GW_SOP/Data/Final_Merged_Noise_Reduced_No_Abs.csv\", header=None)\n",
        "noise = noise_df.values.astype(float)\n",
        "\n",
        "\n",
        "data_BBH_df = pd.read_csv(\"/content/gdrive/MyDrive/GW_SOP/Data/Final_BBH_Merged_Noise_Signal_Reduced_No_ABS.csv\", header=None)\n",
        "data_BBH = data_BBH_df.values.astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmYHuecJw1BB",
        "outputId": "5f2379b5-a96f-454d-b6a5-16f54286afab"
      },
      "outputs": [],
      "source": [
        "X = np.concatenate((noise, data_BBH), axis=0)\n",
        "\n",
        "print(len(noise_df.index))\n",
        "print(X)\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_dSmj6xw1BE",
        "outputId": "ebbe52aa-b078-4cca-f065-4a78a67a96dd"
      },
      "outputs": [],
      "source": [
        "# Alternate way of creating y for the dataset\n",
        " \n",
        "y = [int(i/samples_per_class) for i in range(samples_per_class*no_of_classes)]\n",
        "y = tf.keras.utils.to_categorical(y)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9h1LUqxBQgm"
      },
      "outputs": [],
      "source": [
        "X *= 1e19\n",
        "print(X)\n",
        "\n",
        "# X *= 1e18\n",
        "# print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xirD0GDBQgm",
        "outputId": "907999f7-ba46-4f93-88e5-c4e8b6fe8d26"
      },
      "outputs": [],
      "source": [
        "X = np.expand_dims(X, axis=-1)\n",
        "print(X.shape)\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zxu2uCZyz8kD"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Cqz7kE80mty",
        "outputId": "09d19ddc-c0d4-4eee-9e0c-5ddfd96dca04"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01pIkFC6w1BJ"
      },
      "outputs": [],
      "source": [
        "# resnet model \n",
        "# when tuning start with learning rate->mini_batch_size -> \n",
        "# momentum-> #hidden_units -> # learning_rate_decay -> #layers \n",
        "\n",
        "class Classifier_RESNET:\n",
        "\n",
        "    def __init__(self, output_directory, input_shape, nb_classes, verbose=False, build=True, load_weights=False):\n",
        "        self.output_directory = output_directory\n",
        "        if build == True:\n",
        "            self.model = self.build_model(input_shape, nb_classes)\n",
        "            if (verbose == True):\n",
        "                self.model.summary()\n",
        "            self.verbose = verbose\n",
        "            if load_weights == True:\n",
        "                self.model.load_weights(self.output_directory\n",
        "                                        .replace('resnet_augment', 'resnet')\n",
        "                                        .replace('TSC_itr_augment_x_10', 'TSC_itr_10')\n",
        "                                        + '/model_init.hdf5')\n",
        "            else:\n",
        "                self.model.save_weights(self.output_directory + 'model_init.hdf5')\n",
        "        return\n",
        "\n",
        "    def build_model(self, input_shape, nb_classes):\n",
        "        n_feature_maps = 64\n",
        "\n",
        "        input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "        # BLOCK 1\n",
        "\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=5, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # expand channels for the sum\n",
        "        shortcut_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
        "\n",
        "        # BLOCK 2\n",
        "\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # expand channels for the sum\n",
        "        shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        output_block_2 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
        "\n",
        "        # BLOCK 3\n",
        "\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # no need to expand channels because they are equal\n",
        "        shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n",
        "\n",
        "        output_block_3 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
        "\n",
        "        # FINAL\n",
        "\n",
        "        gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n",
        "\n",
        "        output_layer = keras.layers.Dense(nb_classes, activation='softmax')(gap_layer)\n",
        "\n",
        "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit(self, x_train, y_train, x_val, y_val, BATCH_SIZE = 64, nb_epochs = 5):\n",
        "        if not tf.test.is_gpu_available:\n",
        "            print('error')\n",
        "            exit()\n",
        "        # x_val and y_val are only used to monitor the test loss and NOT for training\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        history = self.model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=nb_epochs, validation_data=(x_val, y_val))\n",
        "\n",
        "        keras.backend.clear_session()\n",
        "\n",
        "        return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2PqOqLjw1BK"
      },
      "outputs": [],
      "source": [
        "RESNET = Classifier_RESNET(directory, (16384, 1), 2, verbose = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxVd6XC9w1BM",
        "outputId": "35808604-3485-4b15-e671-019cec865d9f"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 1\n",
        "\n",
        "history = RESNET.fit(X_train, y_train, X_val, y_val, BATCH_SIZE, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "IqssaFdDw1BN",
        "outputId": "b80a4862-271a-47ab-ba19-4e8b895c1097"
      },
      "outputs": [],
      "source": [
        "acc = [0.] + history.history['accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "\n",
        "plt.figure(figsize=(13, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Categorical Cross Entropy')\n",
        "plt.ylim([0,2])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = RESNET.model.predict(X)\n",
        "y_pred = np.argmax(pred, axis = 1)\n",
        "y_true = np.argmax(y, axis = 1)\n",
        "\n",
        "precision = precision_score(y_true, y_pred, average='binary')\n",
        "print('Precision: %.3f' % precision)\n",
        "\n",
        "recall = recall_score(y_true, y_pred, average='binary')\n",
        "print('Recall: %.3f' % recall)\n",
        "\n",
        "score = f1_score(y_true, y_pred, average='binary')\n",
        "print('F1-Score: %.3f' % score)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "colab_krastev_1dcnn_multi_classification.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "8356d31dd4c50017c60d528a0c40a4935f9d791f54e24c10e32e3a312d5b6882"
    },
    "kernelspec": {
      "display_name": "Python 3.7.6 64-bit ('base': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
